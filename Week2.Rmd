---
title: "Week2"
author: "Marcelo Gomes Gadelha"
date: "18/09/2019"
output: html_document
---

## Week 2 - Data Science Capstone

### Libraries and basic configuration

```{r package_instance,  warning=FALSE}

  suppressPackageStartupMessages(library("NLP"))
  suppressPackageStartupMessages(library("tm"))
  suppressPackageStartupMessages(library("wordcloud"))
  suppressPackageStartupMessages(library("RColorBrewer"))
  suppressPackageStartupMessages(source("functions.R"))
  suppressPackageStartupMessages(library("RWeka"))
  suppressPackageStartupMessages(library("qdap"))
  suppressPackageStartupMessages(library("ggplot2"))
  suppressPackageStartupMessages(library("ngram"))

```

### Exploratory Analysis - Instructions 

    Exploratory analysis - perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora.
    Understand frequencies of words and word pairs - build figures and tables to understand variation in the frequencies of words and word pairs in the data.

    1. Some words are more frequent than others - what are the distributions of word frequencies?
    2. What are the frequencies of 2-grams and 3-grams in the dataset?
    3. How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?
    4. How do you evaluate how many of the words come from foreign languages?
    5. Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to 
    
## Introduction and Conclusions

(i) In this work, I will start showing one way to clean and sample data text of blogs, news, and twitter. After, I will construct a "Corpus" object that is a collection of documents that will support text analysis. For last, I create two functions that help the construction of "NGRAM" scenarios: my_ngram and coverage. 

(ii) The conclusion of this paper is: It is necessary to sample data on personal computers, also stopwords are frequent in all scenarios, and few words are in a lot of phrases.
    
    
### Read Data
```{r reading_data, echo=TRUE}

blogs_en   <- readLines("final/en_US/en_US.blogs.txt",  encoding = "UTF-8")
news_en    <- readLines("final/en_US/en_US.news.txt", encoding = "UTF-8")
twitter_en <- readLines("final/en_US/en_US.twitter.txt", encoding = "UTF-8")


summary(blogs_en)
summary(news_en)
summary(twitter_en)


b <- wordcount(blogs_en)
n <- wordcount(news_en)
t <- wordcount(twitter_en)

barplot(c(b,n,t), names.arg =  c("blog", "news", "twitter"), horiz = TRUE, main="Word Count")

```

### Set seed, sampleing, and clean data and Sampleing

```{r cleannig_data, echo=TRUE}
#set.seed
set.seed(3010)
#sampling
qtd_sample    <- 500
t_blogs_en    <- sample(blogs_en, qtd_sample)
t_news_en     <- sample(news_en, qtd_sample)
t_twitter_en  <- sample(twitter_en, qtd_sample)
t_all         <- c(t_blogs_en, t_news_en, t_twitter_en)
#cleaning
t_all         <- removePunctuation(t_all)
t_all         <- stripWhitespace(t_all)
t_all         <- removeNumbers(t_all)
t_all         <- tolower(t_all)
t_all         <- t_all[which(t_all!="")]

summary(t_all)

```

## 1. Some words are more frequent than others - what are the distributions of word frequencies?


```{r question_1}

modi_txt <- t_all

modi              <- Corpus(VectorSource(modi_txt))
modi_no_stopwords <- tm_map(modi, removeWords, stopwords("english"))

tdm_modi          <- TermDocumentMatrix (modi)
TDM1              <- as.matrix(tdm_modi)
v                 <- sort(rowSums(TDM1), decreasing = TRUE)

print("With stopwords")
wordcloud (modi,  max.words=30,  random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

print("Without stopwords")
wordcloud (modi_no_stopwords, max.words=30,  random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))


my_ngram <-function(x, n){
  
  gramN             <- NGramTokenizer(x)
  gramN             <- NGramTokenizer(gramN, Weka_control(min=n, max=n))
  gramN             <- data.frame(table(gramN))
  gramN             <- gramN[order(gramN$Freq, decreasing = TRUE),]  
  
  return(gramN)
  
}

gramOne <- my_ngram(t_all, 1)

gramOne[1:10,]
par(mfrow=c(1,1))
barplot(gramOne$Freq[1:10], names.arg = as.character(gramOne$gramN[1:10]))

```

## 2. What are the frequencies of 2-grams and 3-grams in the dataset?


```{r question_2}



gramTwo   <- my_ngram(t_all, 2)
gramThree <- my_ngram(t_all, 3)


gramTwo[1:10,]
barplot(gramTwo$Freq[1:10], names.arg = as.character(gramTwo$gramN[1:10]))

gramThree[1:10,]
barplot(gramThree$Freq[1:10], names.arg = as.character(gramThree$gramN[1:10]))

```

## 3. How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?


```{r question_3}

coverage <- function(x, freq){
  
  tmp = 0
  max = trunc(sum(x$Freq) * freq)
  
  
  for(i in 1:nrow(x)){
    
    tmp = x$Freq[i] + tmp
    
    
    if(tmp > max){
      
      return(i)
      
    }
    
  }

}

coverage(gramOne, 0.5)
coverage(gramOne, 0.9)

```

## 4. How do you evaluate how many of the words come from foreign languages?

(i) If a word has accents (á, ã, ü, ç) or ideograms (男 or 男人) the word is not in english language.

(ii) It is possible to use a digital dictionary to verify the language of the word.


## 5. Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to 

(i) Reduce unpopular words.

(ii) Uses a thesaurus to transform similar words.

(iii) Cluster similar words.

    
    
    